datadir="file:/home/training/training_materials/dev1/examples/example-data/"

# read a file with format userid[tab]firstname lastname
userfile = datadir+"userlist.tsv"
users = sc.textFile(userfile) \
    .map(lambda line: line.split('\t')) \
    .map(lambda fields: (fields[0],fields[1]))
users.collect()

# parse a log file with userid in the third field, key by user ID
sc.textFile(logfile).keyBy(lambda line: line.split(' ')[2])

# Read zip code, latitude, longitude from a file, map to (zip,(lat,lon))
zipcoordfile = "file:/home/training/training_materials/sparkdev/data/latlon.tsv"
zipcoords = sc.textFile(zipcoordfile) \
    .map(lambda s: \
      (s.split()[0],(s.split()[1],s.split()[2])))
for (zip,coords) in zipcoords.take(5): print "Zip code: ", zip, " at ", coords

# order file format: orderid:skuid,skuid,skuid...
# map to RDD of skuids keyed by orderid
orderfile = datadir+"orderskus.txt"
orders = sc.textFile(orderfile) \
  .map(lambda s: s.split('\t')) \
  .map(lambda fields: (fields[0],fields[1])) \
  .flatMapValues(lambda skus:skus.split(':'))

for pair in orders.take(5): print pair

# count words in the file
wordfile = datadir+"purplecow.txt"
counts = sc.textFile(wordfile) \
    .flatMap(lambda s: s.split()) \
    .map(lambda w: (w,1)) \
    .reduceByKey(lambda v1,v2: v1+v2)
for pair in counts.take(5): print pair

